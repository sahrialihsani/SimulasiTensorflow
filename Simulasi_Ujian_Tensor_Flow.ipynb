{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simulasi Ujian Tensor Flow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "o09MtTAWYfsC"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNN23zY_-KNT"
      },
      "source": [
        "# Simulasi A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnDiuxXD-Yk_"
      },
      "source": [
        "# Problem A1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbwvTZop-Ek7",
        "outputId": "17d80567-918b-49a0-d044-10e9ad732ce9"
      },
      "source": [
        "# =================================================================================\n",
        "# PROBLEM A1 \n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1].\n",
        "# Do not use lambda layers in your model.\n",
        "# \n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =================================================================================\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_A1():\n",
        "    X = np.array([-4.0, -3.0, -2.0, -1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0], dtype=float)\n",
        "    Y = np.array([5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, ], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    lr= 6e-1\n",
        "    model= tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[1])])\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=30,\n",
        "    mode='auto')\n",
        "\n",
        "    model.fit(X,Y, epochs=500, callbacks=[stop])\n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = solution_A1()\n",
        "    model\n",
        "    model.save(\"model_A1.h5\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 771ms/step - loss: 8.9736 - mse: 97.1534\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.0737 - mse: 74.5026\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.1737 - mse: 59.4118\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 3ms/step - loss: 6.2737 - mse: 51.8809\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.5142 - mse: 51.9099\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.4448 - mse: 52.6876\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 5.0306 - mse: 45.4513\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.3679 - mse: 35.2109\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 3.6110 - mse: 24.8749\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9567 - mse: 16.3063\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5226 - mse: 10.4781\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0839 - mse: 6.8377\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5965 - mse: 4.4128\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0687 - mse: 2.6795\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5958 - mse: 1.3841\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1648 - mse: 0.3300\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0374 - mse: 0.0748\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4054 - mse: 0.8486\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.7230 - mse: 1.6640\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9236 - mse: 2.1372\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0176 - mse: 2.3104\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9998 - mse: 2.3116\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.9170 - mse: 2.3569\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8943 - mse: 2.5317\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.8579 - mse: 2.4323\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6885 - mse: 1.7821\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4269 - mse: 0.9287\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2082 - mse: 0.4163\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1888 - mse: 0.3785\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1555 - mse: 0.3111\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0490 - mse: 0.0980\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0698 - mse: 0.1396\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.2223 - mse: 0.4452\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2972 - mse: 0.5990\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2641 - mse: 0.5281\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2325 - mse: 0.4651\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2407 - mse: 0.4874\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2050 - mse: 0.4131\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1005 - mse: 0.2011\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0293 - mse: 0.0585\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0385 - mse: 0.0770\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0497 - mse: 0.0994\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0329 - mse: 0.0657\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0454 - mse: 0.0908\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0920 - mse: 0.1840\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1072 - mse: 0.2145\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0807 - mse: 0.1613\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0625 - mse: 0.1250\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0594 - mse: 0.1188\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0377 - mse: 0.0753\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0074 - mse: 0.0148\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0040 - mse: 0.0081\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0184 - mse: 0.0367\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0213 - mse: 0.0426\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0205 - mse: 0.0409\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0312 - mse: 0.0624\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0379 - mse: 0.0757\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0273 - mse: 0.0546\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0158 - mse: 0.0316\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0135 - mse: 0.0270\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0091 - mse: 0.0182\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0012 - mse: 0.0025\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0021 - mse: 0.0041\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0091 - mse: 0.0181\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0108 - mse: 0.0215\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0097 - mse: 0.0194\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 0.0119 - mse: 0.0237\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0116 - mse: 0.0232\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0062 - mse: 0.0123\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0027 - mse: 0.0053\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0028 - mse: 0.0057\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018 - mse: 0.0035\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.7068e-04 - mse: 0.0015\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0030 - mse: 0.0061\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0050 - mse: 0.0100\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0042 - mse: 0.0084\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0037 - mse: 0.0073\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0037 - mse: 0.0075\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0021 - mse: 0.0042\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.1840e-04 - mse: 8.3679e-04\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.2805e-04 - mse: 0.0013\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5141e-04 - mse: 0.0019\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.2896e-04 - mse: 0.0015\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014 - mse: 0.0028\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0021 - mse: 0.0042\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 0.0033\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0011 - mse: 0.0022\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0010 - mse: 0.0020\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8379e-04 - mse: 9.6758e-04\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.5435e-06 - mse: 1.9087e-05\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8536e-04 - mse: 5.7071e-04\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.2429e-04 - mse: 0.0010\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.8009e-04 - mse: 9.6019e-04\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9306e-04 - mse: 0.0014\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.8825e-04 - mse: 0.0016\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.5815e-04 - mse: 9.1629e-04\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.6191e-04 - mse: 5.2383e-04\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3366e-04 - mse: 4.6732e-04\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2794e-05 - mse: 1.2559e-04\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4376e-05 - mse: 8.8753e-05\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.3519e-04 - mse: 4.7037e-04\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 2.6956e-04 - mse: 5.3913e-04\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4281e-04 - mse: 4.8563e-04\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9129e-04 - mse: 5.8258e-04\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0558e-04 - mse: 4.1116e-04\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6082e-05 - mse: 1.3216e-04\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0378e-05 - mse: 1.2076e-04\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.2046e-05 - mse: 1.0409e-04\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.7061e-05 - mse: 5.4123e-05\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.5438e-05 - mse: 1.9088e-04\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3514e-04 - mse: 2.7029e-04\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0036e-04 - mse: 2.0072e-04\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.5988e-05 - mse: 1.9198e-04\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4630e-05 - mse: 1.4926e-04\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.6330e-05 - mse: 3.2660e-05\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2619e-05 - mse: 2.5237e-05\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.8692e-05 - mse: 5.7385e-05\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3191e-05 - mse: 4.6381e-05\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 4.4330e-05 - mse: 8.8660e-05\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 5.9864e-05 - mse: 1.1973e-04\n",
            "[[ 7.0085664]\n",
            " [19.00969  ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jh8njZlr-ymp"
      },
      "source": [
        "# Problem A2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lj6Ke-Iq--Ri",
        "outputId": "efd01059-5abc-40ed-b5b4-51b9988597af"
      },
      "source": [
        "# =====================================================================================\n",
        "# PROBLEM A2 \n",
        "#\n",
        "# Build a Neural Network Model for Horse or Human Dataset.\n",
        "# The test will expect it to classify binary classes. \n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ======================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "\n",
        "def solution_A2():\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'                               \n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content/horse-or-human')                  \n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = '/content/validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "    TRAINING_DIR = '/content/horse-or-human'\n",
        "    VAL_DIR = '/content/validation-horse-or-human'\n",
        "   \n",
        "    #train dan test datagen   \n",
        "    train_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    shear_range=0.2,\n",
        "                    zoom_range=0.2,\n",
        "                    horizontal_flip=True\n",
        "                    )\n",
        "    \n",
        "    test_datagen = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    )\n",
        "\n",
        "    #train dan val generator                \n",
        "    train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "    \n",
        "    val_generator = test_datagen.flow_from_directory(VAL_DIR,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=32,\n",
        "        class_mode='binary')\n",
        "\n",
        "    #model\n",
        "    model= tf.keras.models.Sequential([\n",
        "                tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "                tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "                tf.keras.layers.MaxPooling2D(2,2),\n",
        "                tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                tf.keras.layers.MaxPooling2D(2,2),\n",
        "                tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "                tf.keras.layers.MaxPooling2D(2,2),\n",
        "                tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "                tf.keras.layers.MaxPooling2D(2,2),\n",
        "                tf.keras.layers.Flatten(),\n",
        "                tf.keras.layers.Dense(512, activation='relu'),\n",
        "                tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    #compile model\n",
        "    model.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(\n",
        "    learning_rate=0.001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=['accuracy'])\n",
        "    \n",
        "    #train model\n",
        "    model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=8,\n",
        "    epochs=30,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=8,\n",
        ")\n",
        "\n",
        "    return model\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_A2()\n",
        "    model.save(\"model_A2.h5\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n",
            "Epoch 1/30\n",
            "8/8 [==============================] - 11s 1s/step - loss: 0.7220 - accuracy: 0.5242 - val_loss: 0.6810 - val_accuracy: 0.7188\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.8422 - accuracy: 0.6797 - val_loss: 0.5480 - val_accuracy: 0.7188\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.4623 - accuracy: 0.7734 - val_loss: 3.4367 - val_accuracy: 0.5273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27dbwirs-2iq"
      },
      "source": [
        "# Problem A3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8mk0STJ-_I6"
      },
      "source": [
        "# ======================================================================================================\n",
        "# PROBLEM A3 \n",
        "#\n",
        "# Build a classifier for the Human or Horse Dataset with Transfer Learning. \n",
        "# The test will expect it to classify binary classes.\n",
        "# Note that all the layers in the pre-trained model are non-trainable.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The horse-or-human dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "# Inception_v3, pre-trained model used in this problem is developed by Google.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 93%.\n",
        "# =======================================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "\n",
        "def solution_A3():\n",
        "    inceptionv3='https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "    urllib.request.urlretrieve(inceptionv3, 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
        "    local_weights_file = 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "    pre_trained_model = InceptionV3(\n",
        "        input_shape=(150,150,3),\n",
        "        include_top=False,\n",
        "        weights=None)\n",
        "\n",
        "    pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "    for layer in pre_trained_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    last_layer = pre_trained_model.get_layer('mixed7')\n",
        "    last_output = last_layer.output\n",
        "\n",
        "    data_url_1 = 'https://github.com/dicodingacademy/assets/releases/download/release-horse-or-human/horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_1, 'horse-or-human.zip')\n",
        "    local_file = 'horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content/horse-or-human')\n",
        "\n",
        "    data_url_2 = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/validation-horse-or-human.zip'\n",
        "    urllib.request.urlretrieve(data_url_2, 'validation-horse-or-human.zip')\n",
        "    local_file = 'validation-horse-or-human.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content/validation-horse-or-human')\n",
        "    zip_ref.close()\n",
        "\n",
        "    \n",
        "    x = layers.Flatten()(last_output)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    #model pre-trained\n",
        "    model = Model(pre_trained_model.input, x)\n",
        "\n",
        "    #model compile\n",
        "    model.compile(optimizer=RMSprop(lr=0.0001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['acc'])\n",
        "\n",
        "    train_dir = '/content/horse-or-human'\n",
        "    val_dir = '/content/validation-horse-or-human'\n",
        "\n",
        "    #train dan val datagen\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        rotation_range=40,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True\n",
        "    )\n",
        "\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    #train dan validation generator\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )      \n",
        "\n",
        "    validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        batch_size=20,\n",
        "        class_mode='binary',\n",
        "        target_size=(150, 150)\n",
        "    )\n",
        "\n",
        "    #train model\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=51,\n",
        "        epochs=3,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=12,\n",
        "    )\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_A3()\n",
        "    model.save(\"model_A3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk_hLMel-6Fa"
      },
      "source": [
        "# Problem A4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dABYBsLfBCIM"
      },
      "source": [
        "# ==========================================================================================================\n",
        "# PROBLEM A4\n",
        "#\n",
        "# Build and train a binary classifier for the IMDB review dataset.\n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 83%\n",
        "# ===========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.python.keras.preprocessing.text import text_to_word_sequence\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "def solution_A4():\n",
        "    imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "    train_data,test_data=imdb['train'],imdb['test']\n",
        "    # YOUR CODE HERE\n",
        "    #training sentences dan labels\n",
        "    training_sentences=[]\n",
        "    training_labels=[]\n",
        "\n",
        "    #testing sentences dan testing labels\n",
        "    testing_sentences=[]\n",
        "    testing_labels=[]\n",
        "\n",
        "    #pengulangan sententes dan labels pada data latih\n",
        "    for s, l in train_data:\n",
        "        training_sentences.append(s.numpy().decode('utf8'))\n",
        "        training_labels.append(l.numpy())\n",
        "\n",
        "    #pengulangan sententes dan labels pada data testing\n",
        "    for s, l in test_data:\n",
        "        testing_sentences.append(s.numpy().decode('utf8'))\n",
        "        testing_labels.append(l.numpy())\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    training_labels_fix = np.array(training_labels)\n",
        "    testing_labels_fix = np.array(testing_labels)\n",
        "\n",
        "    #inisiasi jumlah kata, dimensi, panjang kata dan tipe token\n",
        "    vocab_size = 10000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    \n",
        "    #inisasi tokenizer, sequences, dan padding pada data latih dan testing\n",
        "    tokenizer =  Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    seq= tokenizer.texts_to_sequences(training_sentences)\n",
        "    padded = pad_sequences(seq, maxlen=max_length, truncating=trunc_type)\n",
        "\n",
        "    test_seq = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(test_seq, maxlen=max_length)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. Do not change the last layer.\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(16)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(6, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    #compile model\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    #training model\n",
        "    model.fit(\n",
        "    padded,\n",
        "    training_labels_fix,\n",
        "    epochs=5,\n",
        "    validation_data=(testing_padded, testing_labels_fix)\n",
        ")\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_A4()\n",
        "    model.save(\"model_A4.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgLvkENp-6ob"
      },
      "source": [
        "# Problem A5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKL-EKerCobv"
      },
      "source": [
        "# =======================================================================================\n",
        "# PROBLEM A5 \n",
        "#\n",
        "# Build and train a neural network model using the Sunspots.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model. \n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from kaggle.com/robervalt/sunspots\n",
        "#\n",
        "# Desired MAE < 0.15 on the normalized dataset.\n",
        "# ========================================================================================\n",
        "\n",
        "import csv\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "\n",
        "# DO NOT CHANGE THIS CODE\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_A5():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sunspots.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'sunspots.csv')\n",
        "\n",
        "    time_step = []\n",
        "    sunspots = []\n",
        "\n",
        "    with open('sunspots.csv') as csvfile:\n",
        "      reader = csv.reader(csvfile, delimiter=',')\n",
        "      next(reader)\n",
        "      for row in reader:\n",
        "        sunspots.append(float(row[2]))\n",
        "        time_step.append(int(row[0]))\n",
        "\n",
        "    series = np.array(sunspots)\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 3000\n",
        "\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 30\n",
        "    batch_size = 32\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size=window_size, batch_size=batch_size, shuffle_buffer=shuffle_buffer_size)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv1D(filters=60, strides=1, padding=\"causal\",\n",
        "                             kernel_size=5,activation=\"relu\",input_shape=[None,1]),\n",
        "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "      tf.keras.layers.Dense(60, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "\n",
        "    # YOUR CODE \n",
        "    #cek model summary\n",
        "    model.summary()\n",
        "    #compile model\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.SGD(learning_rate=1e-4,\n",
        "                                                    momentum=0.9),\n",
        "                  metrics=[\"mae\"])\n",
        "    \n",
        "    #train model\n",
        "    model.fit(train_set, epochs=550)\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_A5()\n",
        "    model.save(\"model_A5.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJbcPudN2QTZ"
      },
      "source": [
        "# Submission B"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGPC_O9s2UmZ"
      },
      "source": [
        "# Problem 1\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n4eABoCq2fIZ"
      },
      "source": [
        "# =============================================================================\n",
        "# PROBLEM B1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-3\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_B1():\n",
        "    X = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0], dtype=float)\n",
        "    Y = np.array([5.0, 7.0, 9.0, 11.0, 13.0, 15.0, 17.0 ], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    lr= 6e-1\n",
        "    model= tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[1])])\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor=\"loss\",\n",
        "    min_delta=0,\n",
        "    patience=30,\n",
        "    mode='min')\n",
        "\n",
        "    model.fit(X,Y, epochs=500, callbacks=[stop])\n",
        "    print(model.predict([-2.0, 10.0])) \n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_B1()\n",
        "    model.save(\"model_B1.h5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_99x2PbqNGlz"
      },
      "source": [
        "# Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fa9fXmjNINL"
      },
      "source": [
        "# =============================================================================\n",
        "# PROBLEM B2\n",
        "#\n",
        "# Build a classifier for the Fashion MNIST dataset.\n",
        "# The test will expect it to classify 10 classes.\n",
        "# The input shape should be 28x28 monochrome. Do not resize the data.\n",
        "# Your input layer should accept (28, 28) as the input shape.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 83%\n",
        "# =============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def solution_B2():\n",
        "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    \n",
        "    #data latih dan data tes\n",
        "    x_train = x_train.reshape(*x_train.shape, 1)\n",
        "    x_test = x_test.reshape(*x_test.shape, 1)\n",
        "\n",
        "    fix_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    fix_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    \n",
        "    #input data\n",
        "    input_shape = next(iter(fix_train))[0].shape\n",
        "    \n",
        "    #batch size\n",
        "    BATCH_SIZE = 128\n",
        "    fix_train = fix_train.batch(BATCH_SIZE)\n",
        "    fix_test = fix_test.batch(BATCH_SIZE)\n",
        "    fix_train\n",
        "    \n",
        "    #callbacks\n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0,\n",
        "        patience=3,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    #train model\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=10,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[stop]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_B2()\n",
        "    model.save(\"model_B2.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pd_c6e9R65g"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyRytjgNR8hq"
      },
      "source": [
        "# ========================================================================================\n",
        "# PROBLEM B3\n",
        "#\n",
        "# Build a CNN based classifier for Rock-Paper-Scissors dataset.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is created by Laurence Moroney (laurencemoroney.com).\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 83%\n",
        "# ========================================================================================\n",
        "\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def solution_B3():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/releases/download/release-rps/rps.zip'\n",
        "    urllib.request.urlretrieve(data_url, 'rps.zip')\n",
        "    local_file = 'rps.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content/')\n",
        "    zip_ref.close()\n",
        "\n",
        "\n",
        "    TRAINING_DIR = \"/content/rps/\"\n",
        "    VAL_DIR = \"/content/rps/\"\n",
        "\n",
        "    training_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                        shear_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True)\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "    train_generator = training_datagen.flow_from_directory(TRAINING_DIR,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=64,\n",
        "        class_mode='categorical')\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(VAL_DIR,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=64,\n",
        "        class_mode='categorical')\n",
        "\n",
        "    stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            min_delta=0,\n",
        "                                            patience=3,\n",
        "                                            mode='min'\n",
        "                                           )\n",
        "\n",
        "    model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "                                        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                                        tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Flatten(),\n",
        "                                        tf.keras.layers.Dropout(0.5),\n",
        "                                        tf.keras.layers.Dense(512, activation='relu'),\n",
        "                                        tf.keras.layers.Dense(3, activation='softmax')\n",
        "                                        ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=10,\n",
        "        steps_per_epoch=train_generator.n//64,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.n//64,\n",
        "        callbacks=[stop]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_B3()\n",
        "    model.save(\"model_B3.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkGFzSIWU0I2"
      },
      "source": [
        "# Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSaErGkpU3Fa",
        "outputId": "382a36c6-0f84-4e94-844a-7c8e01b47bd7"
      },
      "source": [
        "# ===================================================================================================\n",
        "# PROBLEM B4\n",
        "#\n",
        "# Build and train a classifier for the BBC-text dataset.\n",
        "# This is a multiclass classification problem.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in: http://mlg.ucd.ie/datasets/bbc.html.\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 91%\n",
        "# ===================================================================================================\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def solution_B4():\n",
        "    bbc = pd.read_csv('https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/bbc-text.csv')\n",
        "    \n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_portion = .8\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    #hapus dan split kategori\n",
        "    kategori = pd.get_dummies(bbc.category)\n",
        "    bbc_new = pd.concat([bbc, kategori], axis=1)\n",
        "    bbc_new = bbc_new.drop(columns='category')\n",
        "    bbc_new\n",
        "\n",
        "\n",
        "    #input\n",
        "    text = bbc_new['text'].values\n",
        "    labels = bbc_new[['business', 'entertainment', 'politics', 'sport', 'tech']].values\n",
        "\n",
        "    # set of stop words\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "\n",
        "    #bagi data latih dan validasi\n",
        "    training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(text, labels, train_size=training_portion, test_size=0.2,\n",
        "                                                                                              shuffle=False)\n",
        "\n",
        "    #inisasi tokenizer, sequences, dan padding pada data latih dan testing\n",
        "    tokenizer =  Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    tokenizer.word_index\n",
        "    seq= tokenizer.texts_to_sequences(training_sentences)\n",
        "    padded = pad_sequences(seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "    test_seq = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(test_seq,padding=padding_type,maxlen=max_length,truncating=trunc_type)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. Do not change the last layer.\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(30, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    #compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    #training model\n",
        "    model.fit(\n",
        "    padded,\n",
        "    training_labels,\n",
        "    epochs=30,\n",
        "    validation_data=(testing_padded, testing_labels)\n",
        ")\n",
        "\n",
        "    return model\n",
        "\n",
        "    # The code below is to save your model as a .h5 file.\n",
        "    # It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_B4()\n",
        "    model.save(\"model_B4.h5\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "Epoch 1/30\n",
            "56/56 [==============================] - 4s 10ms/step - loss: 1.6011 - accuracy: 0.3169 - val_loss: 1.5881 - val_accuracy: 0.4157\n",
            "Epoch 2/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 1.5649 - accuracy: 0.4337 - val_loss: 1.5326 - val_accuracy: 0.4787\n",
            "Epoch 3/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 1.4688 - accuracy: 0.5320 - val_loss: 1.4020 - val_accuracy: 0.5730\n",
            "Epoch 4/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 1.2924 - accuracy: 0.6067 - val_loss: 1.2130 - val_accuracy: 0.6674\n",
            "Epoch 5/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 1.0809 - accuracy: 0.6994 - val_loss: 1.0246 - val_accuracy: 0.7101\n",
            "Epoch 6/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.8897 - accuracy: 0.7831 - val_loss: 0.8686 - val_accuracy: 0.7843\n",
            "Epoch 7/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.7263 - accuracy: 0.8551 - val_loss: 0.7316 - val_accuracy: 0.8449\n",
            "Epoch 8/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.5872 - accuracy: 0.9112 - val_loss: 0.6161 - val_accuracy: 0.8831\n",
            "Epoch 9/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.4703 - accuracy: 0.9331 - val_loss: 0.5313 - val_accuracy: 0.8944\n",
            "Epoch 10/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.3766 - accuracy: 0.9444 - val_loss: 0.4611 - val_accuracy: 0.8989\n",
            "Epoch 11/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.3053 - accuracy: 0.9489 - val_loss: 0.4172 - val_accuracy: 0.9034\n",
            "Epoch 12/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.2520 - accuracy: 0.9618 - val_loss: 0.3840 - val_accuracy: 0.9079\n",
            "Epoch 13/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.2121 - accuracy: 0.9713 - val_loss: 0.3579 - val_accuracy: 0.9056\n",
            "Epoch 14/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.1785 - accuracy: 0.9775 - val_loss: 0.3385 - val_accuracy: 0.9079\n",
            "Epoch 15/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.1511 - accuracy: 0.9781 - val_loss: 0.3242 - val_accuracy: 0.9146\n",
            "Epoch 16/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.1305 - accuracy: 0.9837 - val_loss: 0.3056 - val_accuracy: 0.9191\n",
            "Epoch 17/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.1131 - accuracy: 0.9837 - val_loss: 0.2955 - val_accuracy: 0.9213\n",
            "Epoch 18/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0979 - accuracy: 0.9876 - val_loss: 0.2812 - val_accuracy: 0.9281\n",
            "Epoch 19/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0856 - accuracy: 0.9916 - val_loss: 0.2790 - val_accuracy: 0.9281\n",
            "Epoch 20/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0753 - accuracy: 0.9944 - val_loss: 0.2705 - val_accuracy: 0.9281\n",
            "Epoch 21/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0663 - accuracy: 0.9944 - val_loss: 0.2669 - val_accuracy: 0.9303\n",
            "Epoch 22/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0586 - accuracy: 0.9966 - val_loss: 0.2665 - val_accuracy: 0.9281\n",
            "Epoch 23/30\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.0515 - accuracy: 0.9972 - val_loss: 0.2634 - val_accuracy: 0.9303\n",
            "Epoch 24/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0461 - accuracy: 0.9989 - val_loss: 0.2588 - val_accuracy: 0.9303\n",
            "Epoch 25/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.2632 - val_accuracy: 0.9258\n",
            "Epoch 26/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.2654 - val_accuracy: 0.9213\n",
            "Epoch 27/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.2617 - val_accuracy: 0.9258\n",
            "Epoch 28/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.2639 - val_accuracy: 0.9258\n",
            "Epoch 29/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.2556 - val_accuracy: 0.9326\n",
            "Epoch 30/30\n",
            "56/56 [==============================] - 0s 7ms/step - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.2580 - val_accuracy: 0.9303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o09MtTAWYfsC"
      },
      "source": [
        "# Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVP17I_aanR4",
        "outputId": "8b2772be-2e44-4854-8fee-e214e8b5a3c2"
      },
      "source": [
        "# ============================================================================================\n",
        "# PROBLEM B5\n",
        "#\n",
        "# Build and train a neural network model using the Daily Max Temperature.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
        "#\n",
        "# Desired MAE < 0.2 on the normalized dataset.\n",
        "# ============================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import urllib\n",
        "\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_B5():\n",
        "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'daily-max-temperatures.csv')\n",
        "\n",
        "    time_step = []\n",
        "    temps = []\n",
        "\n",
        "    with open('daily-max-temperatures.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        next(reader)\n",
        "        step = 0\n",
        "        for row in reader:\n",
        "            temps.append(float(row[1]))\n",
        "            time_step.append(step)\n",
        "            step += 1\n",
        "\n",
        "            series = np.array(temps)\n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 2500\n",
        "    \n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "            \n",
        " \n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 64\n",
        "    batch_size = 256\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "    print(train_set)\n",
        "    print(x_train.shape)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=60, strides=1, padding=\"causal\",kernel_size=5, \n",
        "                               activation=\"relu\", input_shape=[None, 1]),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.Dense(60, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "    \n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"loss\",\n",
        "        min_delta=0,\n",
        "        patience=3,\n",
        "        mode='min'\n",
        "    )\n",
        "    \n",
        "    #compile model\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.Huber(),\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "        metrics=[\"mae\"]\n",
        "        )\n",
        "    \n",
        "    #train model\n",
        "    model.fit(train_set, epochs=100, callbacks=[stop])\n",
        "\n",
        "            # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_B5()\n",
        "    model.save(\"model_B5.h5\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)>\n",
            "(2500,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 37s 62ms/step - loss: 0.0499 - mae: 0.2843\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0468 - mae: 0.2739\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0428 - mae: 0.2597\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0377 - mae: 0.2403\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0306 - mae: 0.2102\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0213 - mae: 0.1615\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0134 - mae: 0.1172\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0120 - mae: 0.1208\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0111 - mae: 0.1133\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0106 - mae: 0.1073\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0102 - mae: 0.1070\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0098 - mae: 0.1039\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0095 - mae: 0.1027\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0092 - mae: 0.1007\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0089 - mae: 0.0993\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0087 - mae: 0.0977\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0084 - mae: 0.0967\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0082 - mae: 0.0954\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0080 - mae: 0.0942\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0078 - mae: 0.0934\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0077 - mae: 0.0924\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0075 - mae: 0.0916\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0074 - mae: 0.0906\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0072 - mae: 0.0900\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0071 - mae: 0.0893\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0070 - mae: 0.0888\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0069 - mae: 0.0883\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.0068 - mae: 0.0877\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0068 - mae: 0.0873\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0067 - mae: 0.0867\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0066 - mae: 0.0864\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0065 - mae: 0.0861\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0065 - mae: 0.0858\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0064 - mae: 0.0854\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0064 - mae: 0.0850\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.0063 - mae: 0.0848\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0063 - mae: 0.0846\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0062 - mae: 0.0843\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0062 - mae: 0.0839\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0061 - mae: 0.0836\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.0061 - mae: 0.0836\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0060 - mae: 0.0833\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.0060 - mae: 0.0830\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0060 - mae: 0.0829\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0059 - mae: 0.0828\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.0059 - mae: 0.0826\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0059 - mae: 0.0824\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0059 - mae: 0.0822\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0058 - mae: 0.0821\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.0058 - mae: 0.0820\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0058 - mae: 0.0817\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0057 - mae: 0.0816\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0057 - mae: 0.0814\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0057 - mae: 0.0813\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0057 - mae: 0.0811\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0057 - mae: 0.0811\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.0056 - mae: 0.0810\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0056 - mae: 0.0808\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0056 - mae: 0.0807\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0056 - mae: 0.0806\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0055 - mae: 0.0804\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0055 - mae: 0.0803\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0055 - mae: 0.0802\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0055 - mae: 0.0800\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0055 - mae: 0.0799\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0054 - mae: 0.0797\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0054 - mae: 0.0797\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 1s 68ms/step - loss: 0.0054 - mae: 0.0795\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0054 - mae: 0.0794\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0054 - mae: 0.0792\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0053 - mae: 0.0791\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0053 - mae: 0.0790\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.0053 - mae: 0.0788\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0053 - mae: 0.0786\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0052 - mae: 0.0784\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.0052 - mae: 0.0782\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0052 - mae: 0.0782\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0052 - mae: 0.0779\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 1s 70ms/step - loss: 0.0051 - mae: 0.0777\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0051 - mae: 0.0776\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0051 - mae: 0.0774\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0051 - mae: 0.0772\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0050 - mae: 0.0770\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0050 - mae: 0.0769\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0050 - mae: 0.0767\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0050 - mae: 0.0764\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0049 - mae: 0.0762\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0049 - mae: 0.0759\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0049 - mae: 0.0758\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0049 - mae: 0.0756\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 1s 64ms/step - loss: 0.0048 - mae: 0.0753\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0048 - mae: 0.0750\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0048 - mae: 0.0747\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0047 - mae: 0.0744\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 1s 67ms/step - loss: 0.0047 - mae: 0.0742\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 1s 63ms/step - loss: 0.0047 - mae: 0.0738\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 1s 66ms/step - loss: 0.0046 - mae: 0.0736\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 1s 69ms/step - loss: 0.0046 - mae: 0.0732\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0046 - mae: 0.0730\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 1s 65ms/step - loss: 0.0046 - mae: 0.0727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpaiAAa751Bo"
      },
      "source": [
        "# Submission C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jojrS7Ff54FM"
      },
      "source": [
        "# Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfFIKqYE521G",
        "outputId": "2dd867e3-e22b-4295-eb2c-01455d54d546"
      },
      "source": [
        "# =============================================================================\n",
        "# PROBLEM C1\n",
        "#\n",
        "# Given two arrays, train a neural network model to match the X to the Y.\n",
        "# Predict the model with new values of X [-2.0, 10.0]\n",
        "# We provide the model prediction, do not change the code.\n",
        "#\n",
        "# The test infrastructure expects a trained model that accepts\n",
        "# an input shape of [1]\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# Desired loss (MSE) < 1e-4\n",
        "# =============================================================================\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "def solution_C1():\n",
        "    X = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0 ], dtype=float)\n",
        "    Y = np.array([0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5 ], dtype=float)\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    #learning rate\n",
        "    lr= 6e-1\n",
        "\n",
        "    #callback\n",
        "    stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
        "                                            min_delta=0,\n",
        "                                            patience=30,\n",
        "                                            mode='min'\n",
        "                                            )\n",
        "\n",
        "    #modelling\n",
        "    model= tf.keras.Sequential([tf.keras.layers.Dense(1, input_shape=[1])])\n",
        "\n",
        "    #compile model\n",
        "    model.compile(loss=tf.keras.losses.Huber(),\n",
        "                  optimizer=tf.keras.optimizers.Adam(lr=lr),\n",
        "                  metrics=['mse'])\n",
        "\n",
        "    #training model\n",
        "    model.fit(X,Y, epochs=500, callbacks=[stop])\n",
        "    print(model.predict([-2.0, 10.0])) \n",
        "    print(model.predict([-2.0, 10.0]))\n",
        "    return model\n",
        "\n",
        "# The code below is to save your model as a .h5 file\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_C1()\n",
        "    model.save(\"model_C1.h5\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 3s 3s/step - loss: 4.6520 - mse: 38.9177\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8777 - mse: 16.7005\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1714 - mse: 3.8520\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1641 - mse: 0.3282\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1906 - mse: 3.2241\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5576 - mse: 5.1575\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4241 - mse: 4.6250\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.9602 - mse: 2.6103\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2985 - mse: 0.6082\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0316 - mse: 0.0632\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5036 - mse: 1.1066\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7894 - mse: 1.9663\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.7748 - mse: 1.8910\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5094 - mse: 1.0870\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.1186 - mse: 0.2371\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0628 - mse: 0.1255\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3159 - mse: 0.6658\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4637 - mse: 1.0549\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.4195 - mse: 0.9299\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2181 - mse: 0.4412\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0214 - mse: 0.0428\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0578 - mse: 0.1156\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.2183 - mse: 0.4373\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2614 - mse: 0.5290\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1517 - mse: 0.3033\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0206 - mse: 0.0412\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0197 - mse: 0.0393\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.1164 - mse: 0.2328\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1635 - mse: 0.3270\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.1027 - mse: 0.2054\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0202 - mse: 0.0404\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0140 - mse: 0.0279\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0710 - mse: 0.1419\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1001 - mse: 0.2002\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0618 - mse: 0.1237\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0101 - mse: 0.0201\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0074 - mse: 0.0147\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0431 - mse: 0.0863\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0586 - mse: 0.1172\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0325 - mse: 0.0650\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0031 - mse: 0.0062\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0073 - mse: 0.0145\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0309 - mse: 0.0618\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0358 - mse: 0.0717\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0163 - mse: 0.0326\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014 - mse: 0.0029\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0091 - mse: 0.0182\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0225 - mse: 0.0450\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0195 - mse: 0.0391\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0054 - mse: 0.0108\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1909e-04 - mse: 0.0012\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0089 - mse: 0.0179\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0144 - mse: 0.0289\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0083 - mse: 0.0166\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.5523e-04 - mse: 0.0013\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024 - mse: 0.0048\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0085 - mse: 0.0170\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0082 - mse: 0.0165\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0025 - mse: 0.0050\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.8170e-04 - mse: 9.6340e-04\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0041 - mse: 0.0081\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0060 - mse: 0.0121\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0030 - mse: 0.0060\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.1430e-04 - mse: 2.2859e-04\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0015 - mse: 0.0030\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0037 - mse: 0.0074\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0027 - mse: 0.0054\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 3.4832e-04 - mse: 6.9664e-04\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.0988e-04 - mse: 0.0010\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0023 - mse: 0.0045\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0022 - mse: 0.0044\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.7391e-04 - mse: 0.0011\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9828e-04 - mse: 3.9656e-04\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0013 - mse: 0.0026\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016 - mse: 0.0031\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1300e-04 - mse: 0.0010\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.2671e-05 - mse: 6.5343e-05\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1036e-04 - mse: 0.0014\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0010 - mse: 0.0021\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0766e-04 - mse: 8.1531e-04\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4344e-05 - mse: 2.8689e-05\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4217e-04 - mse: 8.8435e-04\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2090e-04 - mse: 0.0014\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.2045e-04 - mse: 6.4090e-04\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 2.3855e-05 - mse: 4.7710e-05\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 2.8968e-04 - mse: 5.7935e-04\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.7735e-04 - mse: 9.5470e-04\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 2.0411e-04 - mse: 4.0823e-04\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.1193e-06 - mse: 6.2385e-06\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8685e-04 - mse: 3.7370e-04\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0846e-04 - mse: 6.1691e-04\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2212e-04 - mse: 2.4425e-04\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.6916e-06 - mse: 1.1383e-05\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4306e-04 - mse: 2.8611e-04\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0989e-04 - mse: 4.1978e-04\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 7.3726e-05 - mse: 1.4745e-04\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1509e-05 - mse: 2.3017e-05\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.0991e-04 - mse: 2.1982e-04\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3275e-04 - mse: 2.6549e-04\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.2433e-05 - mse: 6.4867e-05\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.7521e-06 - mse: 1.9504e-05\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 8.1206e-05 - mse: 1.6241e-04\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.9149e-05 - mse: 1.5830e-04\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1856e-05 - mse: 2.3712e-05\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.5951e-05 - mse: 3.1901e-05\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.3707e-05 - mse: 1.2741e-04\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6271e-05 - mse: 9.2542e-05\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.0715e-06 - mse: 8.1431e-06\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.9743e-05 - mse: 3.9486e-05\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5140e-05 - mse: 9.0281e-05\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.1471e-05 - mse: 4.2942e-05\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9354e-07 - mse: 3.8708e-07\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.9416e-05 - mse: 3.8832e-05\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.8748e-05 - mse: 5.7495e-05\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.7961e-06 - mse: 1.5592e-05\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5092e-06 - mse: 5.0184e-06\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 1.8666e-05 - mse: 3.7331e-05\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6857e-05 - mse: 3.3715e-05\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0450e-06 - mse: 4.0899e-06\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 5.3344e-06 - mse: 1.0669e-05\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.4449e-05 - mse: 2.8899e-05\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 7.2274e-06 - mse: 1.4455e-05\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.9918e-08 - mse: 9.9836e-08\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5063e-06 - mse: 1.3013e-05\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.1530e-06 - mse: 1.8306e-05\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.0377e-06 - mse: 4.0755e-06\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.2659e-06 - mse: 2.5318e-06\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.5709e-06 - mse: 1.3142e-05\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.7394e-06 - mse: 9.4787e-06\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.0475e-07 - mse: 6.0949e-07\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.5906e-06 - mse: 5.1812e-06\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6915e-06 - mse: 9.3830e-06\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4036e-06 - mse: 2.8072e-06\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.8847e-07 - mse: 5.7694e-07\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.8827e-06 - mse: 5.7654e-06\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 26ms/step - loss: 2.4086e-06 - mse: 4.8172e-06\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4602e-07 - mse: 2.9205e-07\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1383e-06 - mse: 2.2765e-06\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3351e-06 - mse: 4.6703e-06\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.9139e-07 - mse: 1.5828e-06\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9262e-07 - mse: 3.8524e-07\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.4441e-06 - mse: 2.8882e-06\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1561e-06 - mse: 2.3122e-06\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.5905e-08 - mse: 7.1810e-08\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.6850e-07 - mse: 1.1370e-06\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.0905e-06 - mse: 2.1810e-06\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 2.9715e-07 - mse: 5.9431e-07\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2950e-07 - mse: 2.5899e-07\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.6143e-07 - mse: 1.5229e-06\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.1291e-07 - mse: 1.0258e-06\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4331e-08 - mse: 4.8663e-08\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.6905e-07 - mse: 7.3811e-07\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.0953e-07 - mse: 1.0191e-06\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.6815e-08 - mse: 1.5363e-07\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 1.0707e-07 - mse: 2.1415e-07\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.8622e-07 - mse: 7.7244e-07\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7244e-07 - mse: 3.4487e-07\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7535e-08 - mse: 3.5071e-08\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3902e-07 - mse: 4.7804e-07\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.1552e-07 - mse: 4.3103e-07\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.4498e-08 - mse: 2.8997e-08\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0853e-07 - mse: 2.1705e-07\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8792e-07 - mse: 3.7585e-07\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.7233e-08 - mse: 7.4465e-08\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0750e-08 - mse: 6.1499e-08\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3589e-07 - mse: 2.7179e-07\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.4432e-08 - mse: 1.2886e-07\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1254e-09 - mse: 1.2251e-08\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.5941e-08 - mse: 1.7188e-07\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.4624e-08 - mse: 1.4925e-07\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0303e-09 - mse: 8.0605e-09\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.3046e-08 - mse: 8.6092e-08\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 6.4786e-08 - mse: 1.2957e-07\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 8.7812e-09 - mse: 1.7562e-08\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6184e-08 - mse: 3.2368e-08\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.9670e-08 - mse: 9.9340e-08\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6703e-08 - mse: 3.3407e-08\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.3169e-09 - mse: 1.0634e-08\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.4922e-08 - mse: 6.9844e-08\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0666e-08 - mse: 4.1333e-08\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1774e-09 - mse: 2.3548e-09\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.0857e-08 - mse: 4.1713e-08\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 1.9311e-08 - mse: 3.8621e-08\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6353e-10 - mse: 7.2706e-10\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.1043e-08 - mse: 2.2086e-08\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.6620e-08 - mse: 3.3241e-08\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0577e-09 - mse: 4.1154e-09\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 5.7118e-09 - mse: 1.1424e-08\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3377e-08 - mse: 2.6754e-08\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3319e-09 - mse: 6.6638e-09\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3770e-09 - mse: 4.7541e-09\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4577e-09 - mse: 1.8915e-08\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 3.5662e-09 - mse: 7.1324e-09\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 23ms/step - loss: 6.3271e-10 - mse: 1.2654e-09\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 6.4186e-09 - mse: 1.2837e-08\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7172e-09 - mse: 7.4343e-09\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.6498e-10 - mse: 5.2996e-10\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.3430e-09 - mse: 8.6860e-09\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.4712e-09 - mse: 6.9423e-09\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2882e-10 - mse: 2.5764e-10\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6664e-09 - mse: 5.3329e-09\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8045e-09 - mse: 5.6089e-09\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 24ms/step - loss: 6.2258e-11 - mse: 1.2452e-10\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.5754e-09 - mse: 3.1507e-09\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.2627e-09 - mse: 4.5254e-09\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0332e-10 - mse: 4.0665e-10\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.9968e-10 - mse: 1.9994e-09\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7893e-09 - mse: 3.5786e-09\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6241e-10 - mse: 5.2483e-10\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.6533e-10 - mse: 1.1307e-09\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 33ms/step - loss: 1.3130e-09 - mse: 2.6260e-09\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.3268e-10 - mse: 4.6536e-10\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 3.0646e-10 - mse: 6.1292e-10\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.6658e-10 - mse: 1.9332e-09\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.4331e-10 - mse: 4.8663e-10\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9681e-10 - mse: 3.9363e-10\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 7.2385e-10 - mse: 1.4477e-09\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.2729e-10 - mse: 4.5457e-10\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.1155e-10 - mse: 2.2310e-10\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.1553e-10 - mse: 1.0311e-09\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 1.7247e-10 - mse: 3.4494e-10\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.7744e-11 - mse: 1.1549e-10\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.7092e-10 - mse: 7.4185e-10\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4450e-10 - mse: 2.8900e-10\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.4998e-11 - mse: 8.9996e-11\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7698e-10 - mse: 5.5397e-10\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.1559e-10 - mse: 2.3117e-10\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.7642e-11 - mse: 5.5284e-11\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.9866e-10 - mse: 3.9732e-10\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.1765e-11 - mse: 1.6353e-10\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.6522e-11 - mse: 3.3044e-11\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4135e-10 - mse: 2.8269e-10\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.0923e-11 - mse: 1.2185e-10\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 27ms/step - loss: 1.5709e-11 - mse: 3.1418e-11\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0732e-10 - mse: 2.1464e-10\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.4673e-11 - mse: 8.9345e-11\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.2419e-11 - mse: 2.4839e-11\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.6085e-11 - mse: 1.5217e-10\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.7839e-11 - mse: 5.5679e-11\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.4681e-12 - mse: 1.6936e-11\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.5741e-11 - mse: 1.1148e-10\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.9171e-11 - mse: 3.8343e-11\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 8.9498e-12 - mse: 1.7900e-11\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.2880e-11 - mse: 8.5761e-11\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2996e-11 - mse: 2.5992e-11\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.7398e-12 - mse: 1.5480e-11\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.0693e-11 - mse: 6.1385e-11\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 7.0082e-12 - mse: 1.4016e-11\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 6.4053e-12 - mse: 1.2811e-11\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.2091e-11 - mse: 4.4182e-11\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.7010e-12 - mse: 9.4020e-12\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.8847e-12 - mse: 1.3769e-11\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.6181e-11 - mse: 3.2363e-11\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3756e-12 - mse: 4.7511e-12\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.7120e-12 - mse: 1.1424e-11\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1167e-11 - mse: 2.2334e-11\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.6942e-13 - mse: 1.5388e-12\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.3496e-12 - mse: 1.0699e-11\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9114e-12 - mse: 1.5823e-11\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.0907e-13 - mse: 8.1814e-13\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.1738e-12 - mse: 1.0348e-11\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.7323e-12 - mse: 1.1465e-11\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.8982e-13 - mse: 3.7963e-13\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5589e-12 - mse: 9.1178e-12\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.3804e-12 - mse: 6.7608e-12\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.4813e-14 - mse: 1.0963e-13\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7083e-12 - mse: 7.4165e-12\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.3572e-12 - mse: 4.7145e-12\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.6848e-13 - mse: 5.3697e-13\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.2028e-12 - mse: 6.4055e-12\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3635e-12 - mse: 2.7270e-12\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1211e-13 - mse: 8.2423e-13\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.0547e-12 - mse: 4.1095e-12\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 7.6942e-13 - mse: 1.5388e-12\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.1338e-13 - mse: 8.2677e-13\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8304e-12 - mse: 3.6608e-12\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9437e-13 - mse: 5.8874e-13\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.3365e-13 - mse: 1.2673e-12\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2437e-12 - mse: 2.4874e-12\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0151e-13 - mse: 2.0301e-13\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0775e-13 - mse: 1.4155e-12\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7069e-13 - mse: 1.5414e-12\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0000e+00 - mse: 0.0000e+00\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.9381e-13 - mse: 1.1876e-12\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2580e-13 - mse: 1.0516e-12\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.9691e-14 - mse: 5.9381e-14\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1866e-13 - mse: 1.4373e-12\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1727e-13 - mse: 6.3454e-13\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0887e-13 - mse: 2.1773e-13\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.1338e-13 - mse: 8.2677e-13\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0039e-14 - mse: 1.4008e-13\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.5115e-14 - mse: 1.5023e-13\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.0046e-13 - mse: 6.0092e-13\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2633e-14 - mse: 8.5265e-14\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 2.1215e-13 - mse: 4.2430e-13\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1215e-13 - mse: 4.2430e-13\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.2989e-15 - mse: 6.5979e-15\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 3.1264e-13 - mse: 6.2528e-13\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3906e-13 - mse: 2.7813e-13\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 5.4813e-14 - mse: 1.0963e-13\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2737e-13 - mse: 4.5475e-13\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 4.1618e-14 - mse: 8.3235e-14\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.8263e-14 - mse: 1.3653e-13\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.1978e-13 - mse: 2.3955e-13\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 3.1023e-14 - mse: 6.2046e-14\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0039e-14 - mse: 1.4008e-13\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3981e-13 - mse: 4.7962e-13\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.0452e-15 - mse: 6.0904e-15\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3323e-13 - mse: 2.6645e-13\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 7.5115e-14 - mse: 1.5023e-13\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.0452e-15 - mse: 6.0904e-15\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1978e-13 - mse: 2.3955e-13\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.8112e-14 - mse: 1.1622e-13\n",
            "[[1.1920929e-07]\n",
            " [5.9999990e+00]]\n",
            "[[1.1920929e-07]\n",
            " [5.9999990e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLGT2TgA6EOF"
      },
      "source": [
        "# Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U1EskAh6F34",
        "outputId": "73434c40-181b-4158-d804-f15a68e68915"
      },
      "source": [
        "# =============================================================================\n",
        "# PROBLEM C2\n",
        "#\n",
        "# Create a classifier for the MNIST Handwritten digit dataset.\n",
        "# The test will expect it to classify 10 classes.\n",
        "#\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# Desired accuracy AND validation_accuracy > 91%\n",
        "# =============================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def solution_C2():\n",
        "    mnist = tf.keras.datasets.mnist\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "    \n",
        "    #data latih dan data tes\n",
        "    x_train = x_train.reshape(*x_train.shape, 1)\n",
        "    x_test = x_test.reshape(*x_test.shape, 1)\n",
        "\n",
        "    fix_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    fix_test = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    \n",
        "    #input data\n",
        "    input_shape = next(iter(fix_train))[0].shape\n",
        "    \n",
        "    #batch size\n",
        "    BATCH_SIZE = 128\n",
        "    fix_train = fix_train.batch(BATCH_SIZE)\n",
        "    fix_test = fix_test.batch(BATCH_SIZE)\n",
        "    fix_train\n",
        "    \n",
        "    #callbacks\n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        min_delta=0,\n",
        "        patience=3,\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=input_shape),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "        tf.keras.layers.MaxPool2D(2, 2),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(128, activation='relu'),\n",
        "        tf.keras.layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    #compile the model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    #train model\n",
        "    model.fit(\n",
        "        x_train,\n",
        "        y_train,\n",
        "        epochs=10,\n",
        "        validation_data=(x_test, y_test),\n",
        "        callbacks=[stop]\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    if __name__ == '__main__':\n",
        "        model = solution_C2()\n",
        "        model.save(\"model_C2.h5\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1875/1875 [==============================] - 39s 5ms/step - loss: 2.9802 - accuracy: 0.5393 - val_loss: 0.3891 - val_accuracy: 0.8852\n",
            "Epoch 2/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.6058 - accuracy: 0.8117 - val_loss: 0.1894 - val_accuracy: 0.9408\n",
            "Epoch 3/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3841 - accuracy: 0.8815 - val_loss: 0.1232 - val_accuracy: 0.9614\n",
            "Epoch 4/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2829 - accuracy: 0.9133 - val_loss: 0.0884 - val_accuracy: 0.9724\n",
            "Epoch 5/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2203 - accuracy: 0.9326 - val_loss: 0.0729 - val_accuracy: 0.9786\n",
            "Epoch 6/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1856 - accuracy: 0.9437 - val_loss: 0.0574 - val_accuracy: 0.9824\n",
            "Epoch 7/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1565 - accuracy: 0.9524 - val_loss: 0.0542 - val_accuracy: 0.9828\n",
            "Epoch 8/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1330 - accuracy: 0.9597 - val_loss: 0.0520 - val_accuracy: 0.9837\n",
            "Epoch 9/10\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1188 - accuracy: 0.9641 - val_loss: 0.0429 - val_accuracy: 0.9864\n",
            "Epoch 10/10\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.1081 - accuracy: 0.9676 - val_loss: 0.0387 - val_accuracy: 0.9886\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx38woCQ6rSm"
      },
      "source": [
        "# Problem 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoHhFJ5D6t4p",
        "outputId": "44c57119-58b7-43e7-e70d-863c55cd61bd"
      },
      "source": [
        "# =======================================================================================================\n",
        "# PROBLEM C3\n",
        "#\n",
        "# Build a CNN based classifier for Cats vs Dogs dataset.\n",
        "# Your input layer should accept 150x150 with 3 bytes color as the input shape.\n",
        "# This is unlabeled data, use ImageDataGenerator to automatically label it.\n",
        "# Don't use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is originally published in https://www.kaggle.com/c/dogs-vs-cats/data\n",
        "# \n",
        "# Desired accuracy and validation_accuracy > 72%\n",
        "# ========================================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "\n",
        "def solution_C3():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/cats_and_dogs.zip'\n",
        "    urllib.request.urlretrieve(data_url, 'cats_and_dogs.zip')\n",
        "    local_file = 'cats_and_dogs.zip'\n",
        "    zip_ref = zipfile.ZipFile(local_file, 'r')\n",
        "    zip_ref.extractall('/content')\n",
        "    zip_ref.close()\n",
        "\n",
        "    BASE_DIR = '/content/cats_and_dogs_filtered'\n",
        "    train_dir = os.path.join(BASE_DIR, 'train')\n",
        "    validation_dir = os.path.join(BASE_DIR, 'validation')\n",
        "\n",
        "    #train dan validasi Data generator\n",
        "    training_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                        shear_range=0.2,\n",
        "                                        zoom_range=0.2,\n",
        "                                        horizontal_flip=True)\n",
        "    val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "\n",
        "    train_generator = training_datagen.flow_from_directory(train_dir,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=64,\n",
        "        class_mode='binary')\n",
        "    \n",
        "    val_generator = val_datagen.flow_from_directory(validation_dir,\n",
        "        target_size=(150, 150),  # scaling gambar menjadi 150*150 px\n",
        "        batch_size=64,\n",
        "        class_mode='binary')\n",
        "\n",
        "    #callback\n",
        "    stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                                            min_delta=0,\n",
        "                                            patience=3,\n",
        "                                            mode='min'\n",
        "                                           )\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.models.Sequential([tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "                                        tf.keras.layers.MaxPooling2D(2, 2),\n",
        "                                        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Conv2D(256, (3,3), activation='relu'),\n",
        "                                        tf.keras.layers.MaxPooling2D(2,2),\n",
        "                                        tf.keras.layers.Flatten(),\n",
        "                                        tf.keras.layers.Dropout(0.5),\n",
        "                                        tf.keras.layers.Dense(512, activation='relu'),\n",
        "                                        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "                                        ])\n",
        "    #compile model\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    #training model\n",
        "    model.fit(\n",
        "        train_generator,\n",
        "        epochs=30,\n",
        "        steps_per_epoch=train_generator.n//64,\n",
        "        validation_data=val_generator,\n",
        "        validation_steps=val_generator.n//64,\n",
        "        callbacks=[stop]\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_C3()\n",
        "    model.save(\"model_C3.h5\")\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2000 images belonging to 2 classes.\n",
            "Found 1000 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "31/31 [==============================] - 52s 620ms/step - loss: 0.6941 - accuracy: 0.5005 - val_loss: 0.6926 - val_accuracy: 0.4990\n",
            "Epoch 2/30\n",
            "31/31 [==============================] - 19s 611ms/step - loss: 0.6916 - accuracy: 0.5279 - val_loss: 0.6890 - val_accuracy: 0.5104\n",
            "Epoch 3/30\n",
            "31/31 [==============================] - 19s 610ms/step - loss: 0.6884 - accuracy: 0.5325 - val_loss: 0.6882 - val_accuracy: 0.5010\n",
            "Epoch 4/30\n",
            "31/31 [==============================] - 19s 622ms/step - loss: 0.6874 - accuracy: 0.5506 - val_loss: 0.6784 - val_accuracy: 0.5760\n",
            "Epoch 5/30\n",
            "31/31 [==============================] - 19s 624ms/step - loss: 0.6839 - accuracy: 0.5517 - val_loss: 0.6889 - val_accuracy: 0.5198\n",
            "Epoch 6/30\n",
            "31/31 [==============================] - 19s 619ms/step - loss: 0.6845 - accuracy: 0.5697 - val_loss: 0.6783 - val_accuracy: 0.5510\n",
            "Epoch 7/30\n",
            "31/31 [==============================] - 19s 627ms/step - loss: 0.6702 - accuracy: 0.6090 - val_loss: 0.6600 - val_accuracy: 0.6156\n",
            "Epoch 8/30\n",
            "31/31 [==============================] - 19s 623ms/step - loss: 0.6572 - accuracy: 0.6188 - val_loss: 0.6549 - val_accuracy: 0.6156\n",
            "Epoch 9/30\n",
            "31/31 [==============================] - 19s 624ms/step - loss: 0.6543 - accuracy: 0.6193 - val_loss: 0.6504 - val_accuracy: 0.6135\n",
            "Epoch 10/30\n",
            "31/31 [==============================] - 19s 640ms/step - loss: 0.6394 - accuracy: 0.6358 - val_loss: 0.6303 - val_accuracy: 0.6365\n",
            "Epoch 11/30\n",
            "31/31 [==============================] - 19s 624ms/step - loss: 0.6328 - accuracy: 0.6379 - val_loss: 0.6328 - val_accuracy: 0.6448\n",
            "Epoch 12/30\n",
            "31/31 [==============================] - 19s 625ms/step - loss: 0.6314 - accuracy: 0.6426 - val_loss: 0.6216 - val_accuracy: 0.6417\n",
            "Epoch 13/30\n",
            "31/31 [==============================] - 19s 623ms/step - loss: 0.6188 - accuracy: 0.6653 - val_loss: 0.6171 - val_accuracy: 0.6521\n",
            "Epoch 14/30\n",
            "31/31 [==============================] - 19s 624ms/step - loss: 0.6172 - accuracy: 0.6508 - val_loss: 0.6052 - val_accuracy: 0.6604\n",
            "Epoch 15/30\n",
            "31/31 [==============================] - 19s 621ms/step - loss: 0.5913 - accuracy: 0.6880 - val_loss: 0.5883 - val_accuracy: 0.6885\n",
            "Epoch 16/30\n",
            "31/31 [==============================] - 19s 626ms/step - loss: 0.5899 - accuracy: 0.6920 - val_loss: 0.5790 - val_accuracy: 0.6896\n",
            "Epoch 17/30\n",
            "31/31 [==============================] - 19s 628ms/step - loss: 0.5804 - accuracy: 0.7102 - val_loss: 0.5678 - val_accuracy: 0.6885\n",
            "Epoch 18/30\n",
            "31/31 [==============================] - 19s 627ms/step - loss: 0.5607 - accuracy: 0.7257 - val_loss: 0.5599 - val_accuracy: 0.7063\n",
            "Epoch 19/30\n",
            "31/31 [==============================] - 19s 630ms/step - loss: 0.5648 - accuracy: 0.7132 - val_loss: 0.5816 - val_accuracy: 0.7063\n",
            "Epoch 20/30\n",
            "31/31 [==============================] - 19s 623ms/step - loss: 0.5577 - accuracy: 0.7113 - val_loss: 0.5443 - val_accuracy: 0.7188\n",
            "Epoch 21/30\n",
            "31/31 [==============================] - 19s 616ms/step - loss: 0.5332 - accuracy: 0.7381 - val_loss: 0.6169 - val_accuracy: 0.6615\n",
            "Epoch 22/30\n",
            "31/31 [==============================] - 19s 621ms/step - loss: 0.5265 - accuracy: 0.7469 - val_loss: 0.5322 - val_accuracy: 0.7312\n",
            "Epoch 23/30\n",
            "31/31 [==============================] - 19s 620ms/step - loss: 0.5772 - accuracy: 0.6958 - val_loss: 0.5474 - val_accuracy: 0.7188\n",
            "Epoch 24/30\n",
            "31/31 [==============================] - 19s 621ms/step - loss: 0.5209 - accuracy: 0.7500 - val_loss: 0.5240 - val_accuracy: 0.7396\n",
            "Epoch 25/30\n",
            "31/31 [==============================] - 19s 617ms/step - loss: 0.5292 - accuracy: 0.7412 - val_loss: 0.5091 - val_accuracy: 0.7521\n",
            "Epoch 26/30\n",
            "31/31 [==============================] - 19s 608ms/step - loss: 0.5110 - accuracy: 0.7588 - val_loss: 0.5919 - val_accuracy: 0.6865\n",
            "Epoch 27/30\n",
            "31/31 [==============================] - 19s 610ms/step - loss: 0.5049 - accuracy: 0.7495 - val_loss: 0.5001 - val_accuracy: 0.7635\n",
            "Epoch 28/30\n",
            "31/31 [==============================] - 19s 613ms/step - loss: 0.4985 - accuracy: 0.7583 - val_loss: 0.4999 - val_accuracy: 0.7573\n",
            "Epoch 29/30\n",
            "31/31 [==============================] - 19s 614ms/step - loss: 0.5014 - accuracy: 0.7619 - val_loss: 0.5195 - val_accuracy: 0.7427\n",
            "Epoch 30/30\n",
            "31/31 [==============================] - 19s 618ms/step - loss: 0.4764 - accuracy: 0.7856 - val_loss: 0.5112 - val_accuracy: 0.7427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3nDe0jb8E9p"
      },
      "source": [
        "# Problem 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNRk2GXg8GqO",
        "outputId": "91d65beb-42d4-4c4d-cd46-f2bccb1e8190"
      },
      "source": [
        "# =====================================================================================================\n",
        "# PROBLEM C4 \n",
        "#\n",
        "# Build and train a classifier for the sarcasm dataset. \n",
        "# The classifier should have a final layer with 1 neuron activated by sigmoid.\n",
        "# \n",
        "# Do not use lambda layers in your model.\n",
        "# \n",
        "# Dataset used in this problem is built by Rishabh Misra (https://rishabhmisra.github.io/publications).\n",
        "#\n",
        "# Desired accuracy and validation_accuracy > 75%\n",
        "# =======================================================================================================\n",
        "\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import urllib\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def solution_C4():\n",
        "    data_url = 'https://github.com/dicodingacademy/assets/raw/main/Simulation/machine_learning/sarcasm.json'\n",
        "    urllib.request.urlretrieve(data_url, 'sarcasm.json')\n",
        "\n",
        "    vocab_size = 1000\n",
        "    embedding_dim = 16\n",
        "    max_length = 120\n",
        "    trunc_type='post'\n",
        "    padding_type='post'\n",
        "    oov_tok = \"<OOV>\"\n",
        "    training_size = 20000\n",
        "\n",
        "    with open(\"/content/sarcasm.json\", 'r') as f:\n",
        "        datastore = json.loads(\"[\" + \n",
        "        f.read().replace(\"}\\n{\", \"},\\n{\") + \n",
        "    \"]\")\n",
        "\n",
        "    sentences = []\n",
        "    labels = []\n",
        "\n",
        "    for item in datastore:\n",
        "        sentences.append(item['headline'])\n",
        "        labels.append(item['is_sarcastic'])\n",
        "\n",
        "    #bagi data latih dan validasi\n",
        "    training_sentences = sentences[0:training_size]\n",
        "    testing_sentences = sentences[training_size:]\n",
        "    training_labels = labels[0:training_size]\n",
        "    testing_labels = labels[training_size:]\n",
        "\n",
        "    # #inisasi tokenizer, sequences, dan padding pada data latih dan testing\n",
        "    tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(training_sentences)\n",
        "    word_index = tokenizer.word_index\n",
        "    training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "    training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "    testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "    testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    # Need this block to get it to work with TensorFlow 2.x\n",
        "    training_padded = np.array(training_padded)\n",
        "    training_labels = np.array(training_labels)\n",
        "    testing_padded = np.array(testing_padded)\n",
        "    testing_labels = np.array(testing_labels)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        # YOUR CODE HERE. Do not change the last layer.\n",
        "        tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dropout(0.5),\n",
        "        tf.keras.layers.Dense(30, activation='relu'),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    #compile model\n",
        "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "    #training model\n",
        "    model.fit(training_padded, training_labels, epochs=10, validation_data=(testing_padded, testing_labels))\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_C4()\n",
        "    model.save(\"model_C4.h5\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 94s 134ms/step - loss: 0.4443 - accuracy: 0.7797 - val_loss: 0.3871 - val_accuracy: 0.8243\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 82s 132ms/step - loss: 0.3832 - accuracy: 0.8274 - val_loss: 0.3906 - val_accuracy: 0.8202\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 82s 132ms/step - loss: 0.3509 - accuracy: 0.8397 - val_loss: 0.3828 - val_accuracy: 0.8250\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.3277 - accuracy: 0.8503 - val_loss: 0.3704 - val_accuracy: 0.8302\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.3125 - accuracy: 0.8597 - val_loss: 0.3839 - val_accuracy: 0.8310\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.2993 - accuracy: 0.8703 - val_loss: 0.3840 - val_accuracy: 0.8252\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.2909 - accuracy: 0.8717 - val_loss: 0.3800 - val_accuracy: 0.8320\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.2804 - accuracy: 0.8778 - val_loss: 0.3905 - val_accuracy: 0.8265\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 82s 131ms/step - loss: 0.2714 - accuracy: 0.8801 - val_loss: 0.3979 - val_accuracy: 0.8328\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 82s 132ms/step - loss: 0.2621 - accuracy: 0.8839 - val_loss: 0.3962 - val_accuracy: 0.8314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yy7NXua_BcCl"
      },
      "source": [
        "# Problem 5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufhUdDPFKWN9",
        "outputId": "d1a51651-941f-4a2c-d404-27259e13d057"
      },
      "source": [
        "# ============================================================================================\n",
        "# PROBLEM C5\n",
        "#\n",
        "# Build and train a neural network model using the Daily Min Temperature.csv dataset.\n",
        "# Use MAE as the metrics of your neural network model.\n",
        "# We provided code for normalizing the data. Please do not change the code.\n",
        "# Do not use lambda layers in your model.\n",
        "#\n",
        "# The dataset used in this problem is downloaded from https://github.com/jbrownlee/Datasets\n",
        "#\n",
        "# Desired MAE < 0.19 on the normalized dataset.\n",
        "# ============================================================================================\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import urllib\n",
        "\n",
        "\n",
        "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
        "    series = tf.expand_dims(series, axis=-1)\n",
        "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
        "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n",
        "    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n",
        "    ds = ds.shuffle(shuffle_buffer)\n",
        "    ds = ds.map(lambda w: (w[:-1], w[1:]))\n",
        "    return ds.batch(batch_size).prefetch(1)\n",
        "\n",
        "\n",
        "def solution_C5():\n",
        "    data_url = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv'\n",
        "    urllib.request.urlretrieve(data_url, 'daily-min-temperatures.csv')\n",
        "\n",
        "    time_step = []\n",
        "    temps = []\n",
        "\n",
        "    with open('daily-min-temperatures.csv') as csvfile:\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        step = 0\n",
        "        for row in reader:\n",
        "            temps.append(float(row[1]))\n",
        "            time_step.append(step)\n",
        "            step=step + 1\n",
        "\n",
        "    series = np.array(temps)\n",
        "    time = np.array(time_step)          \n",
        "\n",
        "    # Normalization Function. DO NOT CHANGE THIS CODE\n",
        "    min = np.min(series)\n",
        "    max = np.max(series)\n",
        "    series -= min\n",
        "    series /= max\n",
        "    time = np.array(time_step)\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    split_time = 2500\n",
        "\n",
        "    time_train = time[:split_time]\n",
        "    x_train = series[:split_time]\n",
        "    time_valid = time[split_time:]\n",
        "    x_valid = series[split_time:]\n",
        "\n",
        "    # DO NOT CHANGE THIS CODE\n",
        "    window_size = 64\n",
        "    batch_size = 256\n",
        "    shuffle_buffer_size = 1000\n",
        "\n",
        "    train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n",
        "    print(train_set)\n",
        "    print(x_train.shape)\n",
        "\n",
        "    #modelling\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Conv1D(filters=60, strides=1, padding=\"causal\",kernel_size=5, \n",
        "                               activation=\"relu\", input_shape=[None, 1]),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.LSTM(60, return_sequences=True),\n",
        "        tf.keras.layers.Dense(60, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(30, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(10, activation=\"relu\"),\n",
        "        tf.keras.layers.Dense(1),\n",
        "    ])\n",
        "\n",
        "    #callbacks\n",
        "    stop = tf.keras.callbacks.EarlyStopping(\n",
        "        monitor=\"loss\",\n",
        "        min_delta=0,\n",
        "        patience=3,\n",
        "        mode='min'\n",
        "    )\n",
        "    \n",
        "    #compile model\n",
        "    model.compile(\n",
        "        loss=tf.keras.losses.Huber(),\n",
        "        optimizer=tf.keras.optimizers.Adam(lr=1e-4),\n",
        "        metrics=[\"mae\"]\n",
        "        )\n",
        "    \n",
        "    #train model\n",
        "    model.fit(train_set, epochs=100, callbacks=[stop])\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "    return model\n",
        "\n",
        "\n",
        "# The code below is to save your model as a .h5 file.\n",
        "# It will be saved automatically in your Submission folder.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_C5()\n",
        "    model.save(\"model_C5.h5\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PrefetchDataset shapes: ((None, None, 1), (None, None, 1)), types: (tf.float64, tf.float64)>\n",
            "(2500,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "10/10 [==============================] - 31s 54ms/step - loss: 0.0913 - mae: 0.3991\n",
            "Epoch 2/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0781 - mae: 0.3663\n",
            "Epoch 3/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0666 - mae: 0.3349\n",
            "Epoch 4/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0534 - mae: 0.2944\n",
            "Epoch 5/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0384 - mae: 0.2408\n",
            "Epoch 6/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0232 - mae: 0.1724\n",
            "Epoch 7/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0152 - mae: 0.1297\n",
            "Epoch 8/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0150 - mae: 0.1315\n",
            "Epoch 9/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0138 - mae: 0.1224\n",
            "Epoch 10/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0135 - mae: 0.1209\n",
            "Epoch 11/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0131 - mae: 0.1192\n",
            "Epoch 12/100\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.0127 - mae: 0.1179\n",
            "Epoch 13/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0124 - mae: 0.1164\n",
            "Epoch 14/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0121 - mae: 0.1150\n",
            "Epoch 15/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0118 - mae: 0.1136\n",
            "Epoch 16/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0115 - mae: 0.1123\n",
            "Epoch 17/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0112 - mae: 0.1109\n",
            "Epoch 18/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0109 - mae: 0.1097\n",
            "Epoch 19/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0107 - mae: 0.1086\n",
            "Epoch 20/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0104 - mae: 0.1076\n",
            "Epoch 21/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0102 - mae: 0.1066\n",
            "Epoch 22/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0100 - mae: 0.1056\n",
            "Epoch 23/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0098 - mae: 0.1047\n",
            "Epoch 24/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0096 - mae: 0.1039\n",
            "Epoch 25/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0094 - mae: 0.1031\n",
            "Epoch 26/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0092 - mae: 0.1023\n",
            "Epoch 27/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0091 - mae: 0.1015\n",
            "Epoch 28/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0089 - mae: 0.1007\n",
            "Epoch 29/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0087 - mae: 0.0999\n",
            "Epoch 30/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0086 - mae: 0.0992\n",
            "Epoch 31/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0085 - mae: 0.0986\n",
            "Epoch 32/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0083 - mae: 0.0977\n",
            "Epoch 33/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0082 - mae: 0.0970\n",
            "Epoch 34/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0080 - mae: 0.0964\n",
            "Epoch 35/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0079 - mae: 0.0957\n",
            "Epoch 36/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0078 - mae: 0.0951\n",
            "Epoch 37/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0077 - mae: 0.0946\n",
            "Epoch 38/100\n",
            "10/10 [==============================] - 1s 53ms/step - loss: 0.0076 - mae: 0.0940\n",
            "Epoch 39/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0075 - mae: 0.0935\n",
            "Epoch 40/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0074 - mae: 0.0931\n",
            "Epoch 41/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0073 - mae: 0.0926\n",
            "Epoch 42/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0072 - mae: 0.0922\n",
            "Epoch 43/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0072 - mae: 0.0919\n",
            "Epoch 44/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0071 - mae: 0.0915\n",
            "Epoch 45/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0070 - mae: 0.0912\n",
            "Epoch 46/100\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.0070 - mae: 0.0909\n",
            "Epoch 47/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0069 - mae: 0.0906\n",
            "Epoch 48/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0069 - mae: 0.0902\n",
            "Epoch 49/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0068 - mae: 0.0900\n",
            "Epoch 50/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0068 - mae: 0.0897\n",
            "Epoch 51/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0067 - mae: 0.0894\n",
            "Epoch 52/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0067 - mae: 0.0892\n",
            "Epoch 53/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0066 - mae: 0.0889\n",
            "Epoch 54/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0066 - mae: 0.0887\n",
            "Epoch 55/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0065 - mae: 0.0884\n",
            "Epoch 56/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0065 - mae: 0.0882\n",
            "Epoch 57/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0065 - mae: 0.0880\n",
            "Epoch 58/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0064 - mae: 0.0878\n",
            "Epoch 59/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0064 - mae: 0.0875\n",
            "Epoch 60/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0063 - mae: 0.0873\n",
            "Epoch 61/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0063 - mae: 0.0871\n",
            "Epoch 62/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0063 - mae: 0.0869\n",
            "Epoch 63/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0062 - mae: 0.0867\n",
            "Epoch 64/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0062 - mae: 0.0865\n",
            "Epoch 65/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0062 - mae: 0.0863\n",
            "Epoch 66/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0061 - mae: 0.0861\n",
            "Epoch 67/100\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.0061 - mae: 0.0858\n",
            "Epoch 68/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0061 - mae: 0.0856\n",
            "Epoch 69/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0060 - mae: 0.0854\n",
            "Epoch 70/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0060 - mae: 0.0852\n",
            "Epoch 71/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0060 - mae: 0.0850\n",
            "Epoch 72/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0059 - mae: 0.0848\n",
            "Epoch 73/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0059 - mae: 0.0846\n",
            "Epoch 74/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0059 - mae: 0.0845\n",
            "Epoch 75/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0059 - mae: 0.0842\n",
            "Epoch 76/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0058 - mae: 0.0840\n",
            "Epoch 77/100\n",
            "10/10 [==============================] - 1s 56ms/step - loss: 0.0058 - mae: 0.0839\n",
            "Epoch 78/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0058 - mae: 0.0837\n",
            "Epoch 79/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0057 - mae: 0.0835\n",
            "Epoch 80/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0057 - mae: 0.0833\n",
            "Epoch 81/100\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0057 - mae: 0.0831\n",
            "Epoch 82/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0057 - mae: 0.0829\n",
            "Epoch 83/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0056 - mae: 0.0828\n",
            "Epoch 84/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0056 - mae: 0.0826\n",
            "Epoch 85/100\n",
            "10/10 [==============================] - 1s 55ms/step - loss: 0.0056 - mae: 0.0823\n",
            "Epoch 86/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0056 - mae: 0.0821\n",
            "Epoch 87/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0055 - mae: 0.0819\n",
            "Epoch 88/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0055 - mae: 0.0818\n",
            "Epoch 89/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0055 - mae: 0.0816\n",
            "Epoch 90/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0054 - mae: 0.0814\n",
            "Epoch 91/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0054 - mae: 0.0811\n",
            "Epoch 92/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0054 - mae: 0.0810\n",
            "Epoch 93/100\n",
            "10/10 [==============================] - 1s 57ms/step - loss: 0.0053 - mae: 0.0807\n",
            "Epoch 94/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0053 - mae: 0.0805\n",
            "Epoch 95/100\n",
            "10/10 [==============================] - 1s 61ms/step - loss: 0.0053 - mae: 0.0803\n",
            "Epoch 96/100\n",
            "10/10 [==============================] - 1s 62ms/step - loss: 0.0053 - mae: 0.0800\n",
            "Epoch 97/100\n",
            "10/10 [==============================] - 1s 58ms/step - loss: 0.0052 - mae: 0.0798\n",
            "Epoch 98/100\n",
            "10/10 [==============================] - 1s 60ms/step - loss: 0.0052 - mae: 0.0796\n",
            "Epoch 99/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0052 - mae: 0.0793\n",
            "Epoch 100/100\n",
            "10/10 [==============================] - 1s 59ms/step - loss: 0.0051 - mae: 0.0790\n"
          ]
        }
      ]
    }
  ]
}